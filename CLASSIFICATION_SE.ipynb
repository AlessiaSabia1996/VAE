{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726304da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "import pydicom as dicom\n",
    "from skimage.util import view_as_windows\n",
    "from skimage import exposure, io, filters\n",
    "import scipy\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, Conv2D, ReLU, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.metrics import Recall, Precision\n",
    "from keras.regularizers import l2,l1\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cvs_file(path):\n",
    "    csv_matrix = np.genfromtxt(path, delimiter=',', skip_header=1)\n",
    "    return csv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"your path to BCS-DBT boxes-train-v2.csv\"\n",
    "labels_csv = pd.read_csv(label_path)\n",
    "training = \"your path\"\n",
    "test = \"your path\"\n",
    "validation = \"your path\"\n",
    "extension = '.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels per mass no mass classification\n",
    "def assign_labels(directory):\n",
    "    dataset_tuples = []\n",
    "    coordinates = []\n",
    "    index = 0\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        #PER OGNI IMMAGINE\n",
    "            for file in files:\n",
    "                if file.endswith(extension):\n",
    "                    filepath = os.path.join(subdir, file)\n",
    "                    img = Image.open(filepath)\n",
    "                \n",
    "                    data = np.array(img)\n",
    "                    data = data.astype(np.float32)\n",
    "                    data = tf.expand_dims(data, axis=-1)\n",
    "                    data = tf.convert_to_tensor(data, dtype=(tf.float32))\n",
    "                    paziente = os.path.basename(subdir)\n",
    "                    patient_row = labels_csv[(labels_csv['PatientID'] == paziente)]\n",
    "                    index = index +1\n",
    "\n",
    "                    \n",
    "                    if not patient_row.empty:  # Controlla se la riga Ã¨ stata trovata nel DataFrame\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label= 0\n",
    "                    data_complete = (data/255., label)\n",
    "                    dataset_tuples.append(data_complete)\n",
    "                    print(f'{subdir} e {data_complete[1]}')\n",
    "    return dataset_tuples\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00def35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tuples = assign_labels(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tuples = assign_labels(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuples = assign_labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = [item[0] for item in training_tuples]\n",
    "train_labels = [item[1] for item in training_tuples]\n",
    "\n",
    "validation_images = [item[0] for item in validation_tuples]\n",
    "validation_labels = [item[1] for item in validation_tuples]\n",
    "\n",
    "test_images = [item[0] for item in test_tuples]\n",
    "test_labels = [item[1] for item in test_tuples]\n",
    "\n",
    "# Creazione di un dataset TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b47394",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "validation_dataset = validation_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bdbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {}\n",
    "\n",
    "# Conta le occorrenze di ciascuna classe\n",
    "for label in train_labels:\n",
    "    if label in class_counts:\n",
    "        class_counts[label] += 1\n",
    "    else:\n",
    "        class_counts[label] = 1\n",
    "\n",
    "# Stampa il numero di elementi per ciascuna classe\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"Classe {label}: {count} elementi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, reduction=16, **kwargs):\n",
    "        super(SEBlock, self).__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        self.reduction = reduction\n",
    "        self.pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc1 = tf.keras.layers.Dense(units=self.channels // self.reduction, activation='relu',kernel_regularizer=l1(0.1),kernel_initializer = keras.initializers.HeNormal())\n",
    "        self.fc2 = tf.keras.layers.Dense(units=self.channels, activation='sigmoid',kernel_regularizer=l2(0.1), kernel_initializer = keras.initializers.GlorotNormal())\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.pooling(inputs)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = tf.reshape(x, shape=(-1, 1, 1, self.channels))\n",
    "        return inputs * x\n",
    "\n",
    "def build_cnn_with_se(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    #x = data_augmentation(inputs)\n",
    "    x = layers.Conv2D(16, (7, 7), activation='relu', kernel_regularizer=l2(0.01),kernel_initializer = keras.initializers.HeNormal())(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = SEBlock(channels=16)(x) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(32, (7, 7), activation='relu', kernel_regularizer=l2(0.01),kernel_initializer = keras.initializers.HeNormal())(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = SEBlock(channels=32)(x) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (5, 5), activation='relu', kernel_regularizer=l2(0.01),kernel_initializer = keras.initializers.HeNormal())(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = SEBlock(channels=64)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01),kernel_initializer = keras.initializers.HeNormal())(x)\n",
    "    x = layers.Dropout(0.2)(x)    \n",
    "    x = SEBlock(channels=128)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(8, activation='relu',kernel_initializer = keras.initializers.RandomNormal(),kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(4, activation='relu',kernel_initializer = keras.initializers.RandomNormal(),kernel_regularizer=l2(0.001))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a891651",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100,200,1)\n",
    "num_classes = 1\n",
    "model = build_cnn_with_se(input_shape,num_classes)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', Precision(), Recall() ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e37c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello con early stopping\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=validation_dataset, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(test_dataset)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy {:.2f} %\".format(accuracy))\n",
    "print(\"Test Precision {:.2f} %\".format(precision))\n",
    "print(\"Test Recall {:.2f} %\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd4102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = modello.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c2a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
